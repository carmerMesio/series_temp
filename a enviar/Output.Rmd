---
title: "Proyecto Series Temporales"
author: "David Cardoner & Arnau Mercader"
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,message=FALSE,warning=FALSE,comment='',fig.height = 5,fig.width = 7)
```

\bigskip

\bigskip

\bigskip



En el presente documento se pretende ilustrar y realizar un _forecast_ a partir de datos recogidos de la empresa __Dexma__, empresa dedicada a la eficiencia y gestión energética. Los datos disponibles estan anonimizados, posiblemente de un cliente del sector alimentario. El rango de estos es el año 2017 y su frecuencia es cuarto horaria (96 observaciones/día). Los datos son de alta frecuencia ( _high frequency data_ ), por lo que la metodología clásica ARIMA no es muy recomendable, ya que el número de parámetros necesarios para controlar el proceso puede llegar a ser excesivo y dar estimaciones inestables e ineficientes. Es por eso, que se implementará primero un análisis de factores (TSFA) y con estos se estimará un modelo vectorial autoregresivo (VAR). Para contrastar el modelo VAR, se comparará con una predicción ponderada de un modelo AR y una descomposición del factor de manera univariante.


\bigskip

\bigskip

\bigskip

__Palabras clave__: Análisis de factores, _high frequency data_, modelos autoregresivos AR-VAR, variables latentes.  


\newpage

## Metodología

El análisis factorial se utiliza como método de análisis de variaciones en la respuesta basada en variables latentes no observadas. Este método esta muy ligado al análisis de componentes principales, pero se asume un error general que de entrada no puede ser explicado por los factores. Al aplicar el modelo de factores, cada uno de ellos se puede interpretar como una serie individual. A la vez, este análisis permite caracterizar relaciones y patrones en los datos estudiando el comportamiento de estos. 

Sea $y_t$ un vector m-dimensional de longitud $T$ y $n$ el número de factores o variables latentes ($n << T$) recogidos en el n-vector $\phi$. La relación entre $y_t$ y los factores $\phi$ se asume lineal y se puede caracterizar mediante la expresión: 

$$y_t = \alpha_t + L \phi_t + \epsilon_t$$

Donde $\alpha_t$ juega el papel de _intercept_ y se puede despreciar si los datos $y_t$ estan centrados. La matriz de loadings L es de dimensión $m \;x \;n$ y se considera invariante en el tiempo, y el error aleatorio $\epsilon$ se asume independiente de los factores $\phi$. La matriz L es la que permite pasar de la estimación de los factores a la variable respuesta $y_t$.

El análisis se complementará con la utilización de variables explicativas para intentar mejorar la _performance_ del modelo. Se crearán variables binarias para: caracterizar si el día es domingo, días festivos del año 2017 en España y períodos de festividad clásicos (Semana Santa y Navidad conjuntamente). Estos datos se han obtenido de la página: https://www.calendario-365.es/dias-festivos/2017.html (última consulta 06/06) mediante _scrapping_. El Rdata festivos_2017 proporciona esta información además de la variable respuesta.


### Validación y determinación del número de factores

Para determinar el número de factores se contrastarán distintos enfoques: el Test de Bartlett, autovalores asociados a la matriz de correlaciones (dimensión $310*96$) de la serie multivariante superiores a la unidad y estadísticos como CFI y RMSEA obtenidos mediante la función _FafitStats_ de R. Se reservarán 54 observaciones para validar el poder predictivo del modelo. Como métricas para ver la calidad de estas predicciones se usará RMSE, MAPE y la mediana del APE para intentar corregir el efecto de los domingos (consumo muy inferor) y posibles inestabilidades puntuales en el consumo.

Definición de las métricas:

- RMSE = $\displaystyle \sqrt{n^{-1}\sum_{i=1}^{t}(y_i-\hat{y_i})^2}$
- MAPE = $\displaystyle n^{-1}\sum_{i=1}^{t}\dfrac{|y_i-\hat{y_i}|}{y_i}$
- median(APE) = $\displaystyle med{\dfrac{|y_i-\hat{y_i}|}{y_i}}$


\newpage 

## Análisis de los factores

A continuación se muestra un gráfico de la serie a analizar así como un histograma de la distribución conjunta donde se observa bimodalidad.

```{r,fig.height=4,fig.width=6}
# load packages
library(tsfa)
library(nFactors)
library(vars)
library(readr)
library(foreach)
library(doParallel)
library(lubridate)
library(forecast)
library(knitr)

superm<-read_delim("Quarter Hourly Electrical Energy_01-01-2017_31-12-2017.csv", 
                   ";", escape_double = FALSE, trim_ws = TRUE)

load("tabla_festivos.RData")

param = 310
dif = 364-param

datos=matrix(superm$`General [kWh]`,nrow=param,ncol=96, byrow = T)
predicc=matrix(superm$`General [kWh]`[-c(1:(param*96))],nrow=dif,ncol=96, byrow = T)

##domingos y festivos
valdom=ifelse(weekdays(dmy(superm$Date))=="domingo",1,0)
datdom=matrix(valdom,nrow = param,ncol = 96,byrow = T)[,1]
preddom=matrix(valdom[-c(1:(param*96))],nrow=dif,ncol=96, byrow = T)[,1]

##festivos y fiestas navidad y sem santa
festiusdat=matrix(news$festivo,nrow = param,ncol = 96,byrow = T)[,1]
festiuspred=matrix(news$festivo[-c(1:(param*96))],nrow=dif,ncol=96, byrow = T)[,1]
  
hwfiesdat=matrix(news$holw,nrow=param,ncol=96, byrow = T)[,1]
hwfiespred=matrix(news$holw[-c(1:(param*96))],nrow=dif,ncol=96, byrow = T)[,1]


serie=ts(superm$`General [kWh]`,freq=96)
plot(serie,ylab='Consumo [KWh]',xlab='Tiempo',main='Serie de consumo')

hist(serie,col='steelblue',main='Histograma de la serie de consumo',xlab='Consumo [KWh]',
     ylab='Freq')

```

\newpage

Seguidamente se muestra la validación basada en los _eigenvalues_. El gráfico ilustra el peso o significación de cada una de las columnas de los datos. Según este criterio se deben escoger como factores aquellos valores superiores a la unidad, en este caso, se deben seleccionar 7 factores.

```{r}
zz <- eigen(cor(datos), symmetric=TRUE)[["values"]] 
par(omi=c(0.1,0.1,0.1,0.1),mar=c(4.1,4.1,0.6,0.1)) 
plot(zz, ylab="Value", xlab="Eigenvalue Number", pch=20:20,cex=1,type="o",main="Eigenvalues de la serie")

```


Según el test de Bartlett que también utiliza como _input_ la matriz de correlaciones de los datos, el número de factores resultante debe ser 5. 

```{r}
optnf=nBartlett(cor(datos),N = ncol(datos), alpha=0.05, cor=TRUE, details=TRUE, correction=TRUE)
optnf
```

Como última propuesta se usa la función _FAfitStats_ para ver la significación de los factores en los estadísticos CFI y RMSEA. El CFI ( _comparative fit index_ ) es un pseudo-$R^2$, con rango entre 0 y 1 y el RMSEA ( _root mean square error of approximation_ ) mide la pérdida de ajuste por cada grado de libertad, la distribución de referencia se asume $\chi^2$.

Usando esta propuesta, el número máximo de factores será igual o menor a la cota Ledermann, que en este caso es 82. 


```{r}
# Descomentar si se quiere obtener el output que proporciona el Fafit.RData.
# numfatfit <- FAfitStats(datos, diff.=TRUE, maxit=30000)
load("Fafit.RData")
#output
knitr::kable(t(numfatfit$fitStats[c(5,7),1:15]))
```

Analizando todos los indicadores se decide utilizar 5 factores. Está conclusión se hace en base a la interpretabilidad y también analizando el CFI y RMSEA que muestran un estancamiento a partir del factor 5.

## Estimación de los factores

Para realizar la estimación de los factores se diferenciarán los datos pero no se aplicará normalización. En los siguientes 5 gráficos se puede ver el peso de los factores en la serie. El
factor que más peso tiene es el primero, que podemos interpretar como el consumo durante el tiempo que el establecimiento está abierto de cara al público. El segundo, recoge picos durante la madrugada hasta primera hora de la mañana, quizás necesidades de refrigeración, y los otros tienen un carácter más residual y bastante homogéneo entre los factores 3 y 4, que tienen dos picos, uno por la mañana entre 6 y 10 y otro por la noche sobre las 22.

```{r factors_plots}
# nº of factors
nf= 5
##normalize TRUE o FALSE.
factores<-estTSF.ML(datos,nf,normalize=F,maxit=30000)

hours <- 0:23
labels <- numeric()
for (i in 1:length(hours)){
  labels <- c(labels,rep(c(hours[i],NA),c(1,3)))
}
par(mfrow=c(3,1))
for (i in 1:nf){
  barplot(factores$loadings[,i],main=paste0('Factor ',i),xlab='',cex.names=1,names.arg=labels)
}

```

\newpage

```{r}
# guardamos los factores
fmodel<-factors(factores) #modelo dado por los factores TSFA

```

## Estimación y forecast

En esta sección, con los valores de los 5 factores se estimarán 2 modelos como ya se ha comentado: VAR y predición basada en la media ponderada de AR y descomposición de la serie univariante para cada factor. Se realizarán 2 variantes, una sin las variables exógenas y otra con ellas, para ver su efecto, además de una comparativa entre ambos enfoques.

Para las dos variantes se usará un modelo VAR con los 5 factores con retardo p=1, y estimando un _intercept_. Los modelos AR univariantes se estimarán mediante la función _auto.arima_, que buscará la mejor combinación con restricciones lag máximo p=7 y considerando el proceso estacionario. Para la descomposición de la serie se usará la función _ets_ considerando el error y la tendencia aditiva y no considerando estacionalidad (combinación AAN). Para validar el peso que debemos dar en el forecast a la parte AR y _ets_ se utilizará un _grid_, que ponderará de 0.1 en 0.1, los pesos de cada modelo para crear la combinación lineal que obtenga el mejor valor para los 20 días reservados como validación.

Para llevar a cabo todo este proceso se ha escrito una función R, donde en cada iteración se reestima el modelo TSFA con 5 factores para actualizar así los loadings (matriz L) y factores. En el material adjunto, existen 2 animaciones que permiten ver el progreso del algoritmo en el conjunto de test, simulando un poco los outputs que dan librerías recientes como Tensorflow. En la función se guardan las predicciones para poder evaluar las métricas diarias y globales. 

Como punto final a destacar, se usará la función 2 veces, una sin considerar variables exógenas y otra añadiendo el código adecuado para poder usar exógenas en nuestros modelos.

\newpage

### Estimación sin variables exógenas

A continuación, se muestran las tablas con el valor de la métrica diaria y global generada para los datos de test.

```{r function_bucle}
{
## function bucle
j<-1
datos2 <- datos
datdom2<-datdom
fest2 <- festiusdat
nad2 <- hwfiesdat
p<-vector()
factorspred <- matrix(nrow = nf,ncol = nrow(predicc))
resppred <- matrix(nrow = nrow(predicc),ncol = 96)
resppredarima <- matrix(nrow = nrow(predicc),ncol = 96)
for(i in nrow(datos):363){

factores<-estTSF.ML(datos2,nf,normalize=F,maxit=30000)
fmodel<-factors(factores)
serief1ts <- ts(fmodel)
##Mejor modelo con const.
varf1 <- VAR(serief1ts,p=1, type="const")

##Autoarima with also three covariables auxiliary. And ets with trend of 0.2
prediccarima <- vector()
for(t in 1:ncol(serief1ts)){
vararima <- auto.arima(serief1ts[,t],max.p = 7,max.q = 0,stationary = TRUE)
varets <- as.vector(forecast(ets(serief1ts[,t],model='AAN'),h = 1)[2]$mean)
ab<-predict(vararima)
#prediccarima <- c(prediccarima,ab$pred)
prediccarima <- c(prediccarima,c(.3*ab$pred+.7*varets))
}

resppredarima[j,]<-t(factores$loadings%*%prediccarima)


aa<-predict(varf1,n.ahead = 1)

f1<-aa$fcst$Factor.1[,1]
f2<-aa$fcst$Factor.2[,1]
f3<-aa$fcst$Factor.3[,1]
f4<-aa$fcst$Factor.4[,1]
f5<-aa$fcst$Factor.5[,1]


fac <- t(matrix(c(f1,f2,f3,f4,f5),nrow=length(f1),ncol=5))
factorspred[,j]<-fac
p<-c(p,t(factores$loadings%*%factorspred[,j]))
resppred[j,]<-t(factores$loadings%*%factorspred[,j])
datos2 <- rbind(datos2,predicc[j,])
datdom2 <- c(datdom2,preddom[j])
fest2 <- c(fest2,festiuspred[j])
nad2 <- c(nad2,hwfiespred[j])

# par(mfrow=c(2,2))
# ##plot individual
# plot(ts(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),freq=96),type="l",ylab='',
#      xlab=paste0('Time | RMSE=',round(Metrics::rmse(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),
#                                                         as.vector(t(resppred[j,]))),2)),
#      lty=2,col="blue",
#      ylim=c(0,max(c(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),as.vector(t(resppred[j,]))))))
# title("Individual VARX Model")
# lines(ts(as.vector(t(resppred[j,])),freq=96))
# 
# ##plot colectivo
# plot(ts(as.vector(t(datos2[-c(1:nrow(datos)),])),freq=96),type="l",lty=2,col="blue",ylab='')
# title("VARX Collective Model")
# lines(ts(as.vector(t(resppred)),freq=96))
# 
# ##plot individual arima
# plot(ts(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),freq=96),type="l",ylab='',
#      xlab=paste0('Time | RMSE=',round(Metrics::rmse(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),
#                                                     as.vector(t(resppredarima[j,]))),2)),
#      lty=2,col="red",ylim=c(0,max(c(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),as.vector(t(resppredarima[j,]))))))
# title("Individual Autoarima Model")
# lines(ts(as.vector(t(resppredarima[j,])),freq=96))
# 
# ##plot colectivo arima
# plot(ts(as.vector(t(datos2[-c(1:nrow(datos)),])),freq=96),type="l",lty=2,col="red",ylab='')
# title("Autoarima Model")
# lines(ts(as.vector(t(resppredarima)),freq=96))
j<-j+1

}
}
```


```{r}
#taula
rmse_v <- numeric(54)
mape_v <- numeric(54)
median_ape_v <- numeric(54)
rmse_vv <- numeric(54)
mape_vv <- numeric(54)
median_ape_vv <- numeric(54)

for (i in 1:dim(resppred)[1]) {

  rmse_v[i] <- Metrics::rmse(predicc[i,],resppred[i,])
  mape_v[i] <- Metrics::mape(predicc[i,],resppred[i,])
  median_ape_v[i] <- median(Metrics::ape(predicc[i,],resppred[i,]))
  
  rmse_vv[i] <- Metrics::rmse(predicc[i,],resppredarima[i,])
  mape_vv[i] <- Metrics::mape(predicc[i,],resppredarima[i,])
  median_ape_vv[i] <- median(Metrics::ape(predicc[i,],resppredarima[i,]))
  
  
}

options(digits=4)

taula1 <- data.frame(VAR_rmse = rmse_v, Ens_rmse = rmse_vv,
                     VAR_mape=mape_v, Ens_mape=mape_vv,
                     VAR_median_ape = median_ape_v, Ens_median_ape = median_ape_vv)


kable(taula1,caption = 'Métricas diarias',row.names = TRUE)

taula1g <- data.frame(VAR_rmse = Metrics::rmse(as.vector(t(predicc)),as.vector(t(resppred))),                         Ens_rmse = Metrics::rmse(as.vector(t(predicc)),as.vector(t(resppredarima))),
                     VAR_mape=Metrics::mape(as.vector(t(predicc)),as.vector(t(resppred))), Ens_mape=Metrics::mape(as.vector(t(predicc)),as.vector(t(resppredarima))),
                     VAR_median_ape = median(Metrics::ape(as.vector(t(predicc)),as.vector(t(resppred)))), Ens_median_ape = median(Metrics::ape(as.vector(t(predicc)),as.vector(t(resppredarima)))))

kable(taula1g,caption = 'Métricas globales')


```

\newpage

### Estimación con variables exógenas

Tras realizar los modelos sin variables exógenas se adjuntan seguidamente las tablas obtenidas con el mismo formato que antes pero añadiendo variables exógenas en el modelo.

```{r}
{
  ## function bucle
  j<-1
  datos2 <- datos
  datdom2<-datdom
  fest2 <- festiusdat
  nad2 <- hwfiesdat
  p<-vector()
  factorspred <- matrix(nrow = nf,ncol = nrow(predicc))
  resppred <- matrix(nrow = nrow(predicc),ncol = 96)
  resppredarima <- matrix(nrow = nrow(predicc),ncol = 96)
  for(i in nrow(datos):363){
    
    factores<-estTSF.ML(datos2,nf,normalize=F,maxit=30000)
    fmodel<-factors(factores)
    serief1ts <- ts(fmodel)
    ##Mejor modelo con const.
    varf1 <- VAR(serief1ts,p=1, type="const",exogen = as.matrix(cbind(dat=datdom2,fest=fest2,nad=nad2)))
    
    ##Autoarima with also three covariables auxiliary. And ets with trend of 0.2
    prediccarima <- vector()
    for(t in 1:ncol(serief1ts)){
      vararima <- auto.arima(serief1ts[,t],max.p = 7,max.q = 0,stationary = TRUE,xreg = cbind(dat=datdom2,fest=fest2,nad=nad2))
      varets <- as.vector(forecast(ets(serief1ts[,t],model='AAN'),h = 1)[2]$mean)
      ab<-predict(vararima,newxreg = cbind(preddom[j],festiuspred[j],hwfiespred[j]))
      #prediccarima <- c(prediccarima,ab$pred)
      prediccarima <- c(prediccarima,c(1*ab$pred+0*varets))
    }
    
    resppredarima[j,]<-t(factores$loadings%*%prediccarima)
    
    
    aa<-predict(varf1,dumvar = as.matrix(cbind(preddom[j],festiuspred[j],hwfiespred[j])),n.ahead = 1)
    
    f1<-aa$fcst$Factor.1[,1]
    f2<-aa$fcst$Factor.2[,1]
    f3<-aa$fcst$Factor.3[,1]
    f4<-aa$fcst$Factor.4[,1]
    f5<-aa$fcst$Factor.5[,1]
    
    
    fac <- t(matrix(c(f1,f2,f3,f4,f5),nrow=length(f1),ncol=5))
    factorspred[,j]<-fac
    p<-c(p,t(factores$loadings%*%factorspred[,j]))
    resppred[j,]<-t(factores$loadings%*%factorspred[,j])
    # print(j)
    datos2 <- rbind(datos2,predicc[j,])
    datdom2 <- c(datdom2,preddom[j])
    fest2 <- c(fest2,festiuspred[j])
    nad2 <- c(nad2,hwfiespred[j])
    
    # par(mfrow=c(2,2))
    # ##plot individual
    # plot(ts(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),freq=96),type="l",ylab='',
    #      xlab=paste0('Time | RMSE=',round(Metrics::rmse(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),
    #                                                     as.vector(t(resppred[j,]))),2)),
    #      lty=2,col="blue",
    #      ylim=c(0,max(c(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),as.vector(t(resppred[j,]))))))
    # title("Individual VARX Model")
    # lines(ts(as.vector(t(resppred[j,])),freq=96))
    # 
    # ##plot colectivo
    # plot(ts(as.vector(t(datos2[-c(1:nrow(datos)),])),freq=96),type="l",lty=2,col="blue",ylab='')
    # title("VARX Collective Model")
    # lines(ts(as.vector(t(resppred)),freq=96))
    # 
    # ##plot individual arima
    # plot(ts(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),freq=96),type="l",ylab='',
    #      lty=2,col="red",ylim=c(0,max(c(as.vector(t(datos2[-c(1:(nrow(datos2)-1)),])),as.vector(t(resppredarima[j,]))))))
    # title("Individual Autoarima Model")
    # lines(ts(as.vector(t(resppredarima[j,])),freq=96))
    # 
    # ##plot colectivo arima
    # plot(ts(as.vector(t(datos2[-c(1:nrow(datos)),])),freq=96),type="l",lty=2,col="red",ylab='')
    # title("Autoarima Model")
    # lines(ts(as.vector(t(resppredarima)),freq=96))
    j<-j+1
    
  }
}

```



```{r}
#taula amb exogenes
rmse_v <- numeric(54)
mape_v <- numeric(54)
median_ape_v <- numeric(54)
rmse_vv <- numeric(54)
mape_vv <- numeric(54)
median_ape_vv <- numeric(54)

for (i in 1:dim(resppred)[1]) {

  rmse_v[i] <- Metrics::rmse(predicc[i,],resppred[i,])
  mape_v[i] <- Metrics::mape(predicc[i,],resppred[i,])
  median_ape_v[i] <- median(Metrics::ape(predicc[i,],resppred[i,]))
  
  rmse_vv[i] <- Metrics::rmse(predicc[i,],resppredarima[i,])
  mape_vv[i] <- Metrics::mape(predicc[i,],resppredarima[i,])
  median_ape_vv[i] <- median(Metrics::ape(predicc[i,],resppredarima[i,]))
  
  
}

options(digits=3)

taula1 <- data.frame(VAR_rmse = rmse_v, Ens_rmse = rmse_vv,
                     VAR_mape=mape_v, Ens_mape=mape_vv,
                     VAR_median_ape = median_ape_v, Ens_median_ape = median_ape_vv)


kable(taula1,caption = 'Métricas diarias',row.names = TRUE)

taula1g <- data.frame(VAR_rmse = Metrics::rmse(as.vector(t(predicc)),as.vector(t(resppred))),                         Ens_rmse = Metrics::rmse(as.vector(t(predicc)),as.vector(t(resppredarima))),
                     VAR_mape=Metrics::mape(as.vector(t(predicc)),as.vector(t(resppred))), Ens_mape=Metrics::mape(as.vector(t(predicc)),as.vector(t(resppredarima))),
                     VAR_median_ape = median(Metrics::ape(as.vector(t(predicc)),as.vector(t(resppred)))), Ens_median_ape = median(Metrics::ape(as.vector(t(predicc)),as.vector(t(resppredarima)))))

kable(taula1g,caption = 'Métricas globales')


```

\newpage 

### Conclusiones

- Añadiendo variables exógenas al modelo la calidad predictiva mejora moderadamente, debido, por ejemplo al factor domingo, donde el establecimiento esta cerrado y solo se observa un consumo constante debido a la refrigeración del local (por su sector de actividad).

- El enfoque _Ensemble_ sin considerar variables auxiliares da más peso a la predicción _ets_. Sin embargo, al usarlas la predicción solo da peso a los AR univariantes ya que proporcionan más información.

- El hecho de utilizar un modelo VAR con un retardo o modelos AR univariantes mezclados con descomposición dan en los dos escenarios analizados resultados similares. 

- Podríamos haber enfocado esta predicción como un claro de ejemplo de aplicación de modelos de Machine learning, como XGboost, Random Forest o Redes Neuronales. La idea de reducir la dimensionalidad de los datos mediante factores es rica por su simplez y efectividad, a la vista de los resultados obtenidos. Quizás la parte más compleja es determinar que factores usar y como predecirlos con tal de obtener buenas predicciones. Es en esta última parte donde quizás es bueno combinar una herramienta más clásica o estadística con un enfoque más computacional y moderno como podría ser algun algoritmo de machine learning mencionado arriba.

- Aprender esta metodología ha sido enriquecedor para nosotros y nos ha permitido ver una idea _parecida_ al PCA, el cual si habíamos trabajado. Los factores latentes o subyacentes a la serie permiten añadiendo otras variables exógenas como por ejemplo la variable que permite controlar los domingos, generar una predicción bastante ajustada de la serie.

- El hecho de realizar el forecast para los días de navidad genera un handicap adicional provocado básicamente por el aumento de consumo de los hogares. Por ese motivo, se ha añadido la variable festividad que intentará recoger este aumento. Este factor considera como festividades los días de Semana Santa y el periodo de Navidad por igual.

- Como aspectos a mejorar, para seleccionar el peso de la combinación lineal se podrían haber usado técnicas más complejas como k-fold por bloques de tiempo o un rango más amplio de días. La utilización del método _ets_ ha sido añadido para poder discutir un método tratado en clase.
